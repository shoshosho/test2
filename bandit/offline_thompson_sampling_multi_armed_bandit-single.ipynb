{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import scipy.stats as ss\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import multiprocessing as mp\n",
    "import time \n",
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ts_policy(arms, time, num_sampling, get_reward, no_reward, alpha=1, beta=1):\n",
    "# def ts_policy(arms, time, num_sampling, get_reward, no_reward, alpha=1, beta=0.1):\n",
    "# def ts_policy(arms, times, num_sampling, get_reward, no_reward, alpha=2.1, beta=1.1):\n",
    "#     arms, times, num_sampling = 100,1000,1000\n",
    "    #プレイされたラウンド分の腕ごとの報酬、reward_lisを格納\n",
    "    history_reward = []\n",
    "    #表示された腕、arm_lisを格納\n",
    "    history_arm = []\n",
    "    #各ラウンドでの最大クリック回数、ポアソン分布(mu=1)に従うと仮定\n",
    "    play_limit = []\n",
    "    #play_limitが与えられた前提での最大報酬、高いものから引かれる前提\n",
    "    ideal_rewards = []\n",
    "    #１ラウンド中の合計報酬\n",
    "    num_reward = []    \n",
    "    \n",
    "    #50万回実施のパラメータは固定で与えてあげる\n",
    "    gr = get_reward\n",
    "    nr = no_reward\n",
    "    \n",
    "#     print(\"trial start\")\n",
    "#     print(\"this time given rewards\",get_reward)\n",
    "#     print(\"this time given failures\",no_reward)\n",
    "    \n",
    "    #上記２種のリストをthompson sampling計算用のパラメータに直したバージョン\n",
    "    adjusted_reward = np.zeros(arms)\n",
    "    adjusted_no_reward = np.zeros(arms)        \n",
    "    #各腕ごとに異なる標準偏差を持っている\n",
    "    sd = (np.random.rand(100,)/100).tolist()\n",
    "    \n",
    "#     proc = num_process\n",
    "    #multiprocess対応するため、作業範囲を切っておく\n",
    "\n",
    "    for t in range(times):\n",
    "        #TSした時の報酬を格納しておく場所\n",
    "        mu = np.zeros(arms)\n",
    "        for k in range(arms):\n",
    "            #パラメータの上限を設定しておかないと、特定の腕だけが飛び抜けて選択されるようになる\n",
    "            #上限に達した後は、1000の中で按分するようにすればうまくいくのでは？\n",
    "            played = gr[k]+nr[k]\n",
    "            if played >1000:\n",
    "                adjusted_reward[k] = (gr[k]/played)*1000\n",
    "                adjusted_no_reward[k] = (nr[k]/played)*1000\n",
    "            else:\n",
    "                adjusted_reward[k] = gr[k]\n",
    "                adjusted_no_reward[k] = nr[k]\n",
    "                \n",
    "            #ベータ分布の形をした報酬から報酬をピックアップする\n",
    "            mu[k] = np.random.beta(adjusted_reward[k]+alpha, adjusted_no_reward[k]+beta, size=num_sampling).sum()\n",
    "#             mu[k] = np.random.beta(np.log(adjusted_reward[k]+alpha), np.log(adjusted_no_reward[k]+beta), size=num_sampling).sum()\n",
    "        #報酬の最も高い中で10個の腕が選択される(zexy仕様)\n",
    "        pulling_arm = np.array(mu).argsort()[-10:][::-1]\n",
    "        \n",
    "        #腕をプレイして報酬を確認、1ならget_rewardに+1、0ならno_rewardに+1\n",
    "        #実際には100で割るんでなくて、1000~10000くらいで割るのが現実的\n",
    "        #1~3までの分布があるものを想定して、そこまでは通常通り実施、あとは報酬0にする\n",
    "        #平均1のポアソン分布に従うプレイ回数とする\n",
    "        limit = ss.poisson.rvs(1)\n",
    "        play_limit.append(limit)\n",
    "        play_per_person = 0\n",
    "        im = list(range(100))\n",
    "        ideal_reward = 0.05*limit - sum(im[:limit])*0.0005\n",
    "        ideal_rewards.append(ideal_reward)\n",
    "        #確率が高いものから選択される、という仮定を置いているが、今のケースだとこれが限界\n",
    "        #contextualを入れればそこはよしなに出しわけ可能\n",
    "        #CTRベースの計算なので、CVRをターゲットとするときはrewardのmaxを1に変える必要ある\n",
    "\n",
    "        #１ラウンド中、腕ごとの報酬\n",
    "        reward_lis = []\n",
    "        for pa in pulling_arm:\n",
    "            if limit > play_per_person:\n",
    "                #腕の報酬部分を設定、max5%~min0.05%、0.05%刻みの100種類\n",
    "                prob = min(abs(np.random.normal((pa+1)/2000,sd[pa])),1)\n",
    "                reward = np.random.choice([0, 1], p=[(1-prob), prob])\n",
    "                #multiでクリックされる想定、複数クリックがあれば複数更新される\n",
    "                #択一のケースも想定する必要あり\n",
    "                if reward ==1:\n",
    "                    get_reward[pa] +=1\n",
    "                else:\n",
    "                    no_reward[pa] +=1\n",
    "                reward_lis.append(reward)\n",
    "            else:\n",
    "                reward_lis.append(0)\n",
    "                no_reward[pa] +=1\n",
    "            play_per_person +=1\n",
    "                \n",
    "            #１人がプレイする上限回数に達したら、残りの報酬は０\n",
    "            \n",
    "        #history、結果の保持\n",
    "        #腕ごと報酬\n",
    "        history_reward.append(reward_lis)\n",
    "        #ラウンドの報酬\n",
    "        num_reward.append(sum(reward_lis))\n",
    "        #プレイされた腕\n",
    "        history_arm.append(pulling_arm)\n",
    "#         #パラメータ調整\n",
    "        \n",
    "        \n",
    "    return [get_reward, no_reward,get_reward+no_reward, history_reward, history_arm,play_limit,ideal_rewards,num_reward]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def wrapper_hiki(tuples):\n",
    "#     return tuples[0](tuples[1],tuples[2],tuples[3],tuples[4],tuples[5],tuples[6])\n",
    "#10000で2分、10万で20分、50万で100分(7ループ)\n",
    "\n",
    "\n",
    "arms = 100\n",
    "times = 100000\n",
    "num_sampling = 1000\n",
    "\n",
    "for tri in range(2): #実験回数\n",
    "    for i in range(3): #どの程度の長さまで見るか(アクセス数*7日が望ましい？)\n",
    "        start=time.time()\n",
    "        if i ==0:\n",
    "            get_reward = np.zeros(arms)\n",
    "            no_reward = np.zeros(arms)\n",
    "\n",
    "        get_reward, no_reward, num_trials, history_reward, history_arm,play_limit,ideal_rewards,num_reward = ts_policy(arms, times, num_sampling, get_reward, no_reward)\n",
    "\n",
    "        if i ==0:\n",
    "            history_reward_big = history_reward\n",
    "            history_arm_big = history_arm\n",
    "            play_limit_big = play_limit\n",
    "            ideal_rewards_big = ideal_rewards\n",
    "            num_reward_big = num_reward\n",
    "        else:\n",
    "            history_reward_big.extend(history_reward)\n",
    "            history_arm_big.extend(history_arm)\n",
    "            play_limit_big.extend(play_limit)\n",
    "            ideal_rewards_big.extend(ideal_rewards)\n",
    "            num_reward_big.extend(num_reward)\n",
    "\n",
    "        end = time.time()\n",
    "        taken_time = end - start\n",
    "        print(taken_time)\n",
    "    \n",
    "    if tri ==0:\n",
    "        total_rewards = np.array(history_reward_big)\n",
    "        total_history_arm = []\n",
    "        total_history_arm.append(history_arm_big)\n",
    "        total_play_limit = np.array(play_limit_big)\n",
    "        total_ideal_rewards= np.array(ideal_rewards_big)\n",
    "        total_num_reward= np.array(num_reward_big)\n",
    "        get_reward = np.array(get_reward)\n",
    "        no_reward = np.array(no_reward)\n",
    "    else:\n",
    "        total_rewards = total_rewards + np.array(history_reward_big)\n",
    "        total_history_arm.append(history_arm_big)\n",
    "        total_play_limit = total_play_limit + np.array(play_limit_big)\n",
    "        total_ideal_rewards= total_ideal_rewards + np.array(ideal_rewards_big)\n",
    "        total_num_reward= total_num_reward + np.array(num_reward_big)\n",
    "        total_get_reward = get_reward + np.array(get_reward)\n",
    "        total_no_reward = no_reward + np.array(no_reward)\n",
    "        \n",
    "    #全部足して試行回数で割る(平均)\n",
    "    # get_reward, no_reward, num_reward_big, ideal_rewards_big\n",
    "    # history_arm_big(extendではなく、リストの連結でOK)\n",
    "    # history_reward_big, \n",
    "\n",
    "\n",
    "        #callbackの中身は\n",
    "        # get_reward, no_reward,get_reward+no_reward, history_reward, history_arm,play_limit,ideal_rewards,num_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(history_reward_big))\n",
    "print(len(history_arm_big))\n",
    "print(len(play_limit_big))\n",
    "print(len(ideal_rewards_big))\n",
    "print(len(num_reward_big))\n",
    "print(len(get_reward))\n",
    "print(len(no_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(total_rewards))\n",
    "print(len(total_history_arm))\n",
    "print(len(total_play_limit))\n",
    "print(len(total_ideal_rewards))\n",
    "print(len(total_num_reward))\n",
    "print(len(total_get_reward))\n",
    "print(len(total_no_reward))\n",
    "\n",
    "total_rewards[:10]\n",
    "\n",
    "# print(sum(history_reward_big))\n",
    "# print(sum(total_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#バッチだと極端に寄っていくので、どうするか悩みどころ\n",
    "print(\"試行回数\",len(history_reward_big))\n",
    "print(\"クリック回数\",sum(play_limit_big))\n",
    "print(\"理想値\",sum(ideal_rewards_big))\n",
    "print(\"実績値\",sum(num_reward_big))\n",
    "print(\"報酬分布\",get_reward)\n",
    "print(\"報酬総数\",sum(get_reward))\n",
    "print(\"失敗分布\",no_reward)\n",
    "print(\"失敗総数\",sum(no_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (3):\n",
    "    mn = i*100000\n",
    "    mx = mn+100000\n",
    "    plt.hist(np.array(history_arm_big[mn:mx]).flatten(),bins=100)\n",
    "    titles = \"playされたarmの分布\"+str(i+1)+\"週目\"\n",
    "    plt.title(titles)\n",
    "    plt.show()\n",
    "\n",
    "plt.hist(np.array(history_arm_big).flatten(),bins=100)\n",
    "titles = \"playされたarmの分布全体\"\n",
    "plt.title(titles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#実験結果\n",
    "plt.plot(np.linspace(0, 1, len(history_reward_big)),np.cumsum(num_reward_big))\n",
    "#最適腕を引き続けた場合\n",
    "plt.plot(np.linspace(0, 1, len(history_reward_big)),np.cumsum(ideal_rewards_big))\n",
    "plt.title(\"実験結果と最適腕を引き続けた場合の報酬\")\n",
    "#regret\n",
    "plt.show()\n",
    "\n",
    "#regretの曲線は、history_armsを適切な値で割った数値(idealと現実の差分)\n",
    "# regret = (np.zeros(len(history_reward_big))+0.05)-np.array(np.array(history_arm_big).flatten())/2000\n",
    "regret = np.array(ideal_rewards_big) - np.array(num_reward_big)\n",
    "cumulative_regret = np.cumsum(regret)\n",
    "plt.plot(np.linspace(0, 1, len(history_reward_big)), cumulative_regret)\n",
    "plt.title(\"regret曲線\")\n",
    "plt.show()\n",
    "\n",
    "#登場回数の多い３位まで\n",
    "print(Counter(np.array(history_arm_big).flatten()).most_common(3))\n",
    "#alpha+beta <=1000,超えたら1000の中で按分\n",
    "#\n",
    "#報酬max5%, 報酬分布の標準偏差0.01 ~ 0.001、alpha,betaはそのまま成功、失敗回数を使う、初期値はそれぞれ1,0.1\n",
    "#同じセッティングで10万イテレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_arm_big[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
