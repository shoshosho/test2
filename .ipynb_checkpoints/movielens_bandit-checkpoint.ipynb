{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Contextual bandit on MovieLens\n",
    "\n",
    "\n",
    "The script uses real-world data to conduct contextual bandit experiments. Here we use\n",
    "MovieLens 10M Dataset, which is released by GroupLens at 1/2009. Please fist pre-process\n",
    "datasets (use \"movielens_preprocess.py\"), and then you can run this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,20) and (18,1) not aligned: 20 (dim 1) != 18 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3c177ec475dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-3c177ec475dc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         seq_error = policy_evaluation(policy, bandit, streaming_batch_small, user_feature, reward_list,\n\u001b[0;32m--> 167\u001b[0;31m                                       actions, action_context)\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mregret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregret_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreaming_batch_small\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3c177ec475dc>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(policy, bandit, streaming_batch, user_feature, reward_list, actions, action_context)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maction_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mfull_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mhistory_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mwatched_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstreaming_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/01017387/anaconda/lib/python3.6/site-packages/striatum/bandit/linucb.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, context, n_actions)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mestimated_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linucb_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/01017387/anaconda/lib/python3.6/site-packages/striatum/bandit/linucb.py\u001b[0m in \u001b[0;36m_linucb_score\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0maction_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             estimated_reward[action_id] = float(\n\u001b[0;32m---> 99\u001b[0;31m                 theta[action_id].T.dot(action_context))\n\u001b[0m\u001b[1;32m    100\u001b[0m             uncertainty[action_id] = float(\n\u001b[1;32m    101\u001b[0m                 self.alpha * np.sqrt(action_context.T\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,20) and (18,1) not aligned: 20 (dim 1) != 18 (dim 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from striatum.storage import history\n",
    "from striatum.storage import model\n",
    "from striatum.bandit import ucb1\n",
    "from striatum.bandit import linucb\n",
    "from striatum.bandit import linthompsamp\n",
    "from striatum.bandit import exp4p\n",
    "from striatum.bandit import exp3\n",
    "# from striatum.bandit.bandit import Action\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    streaming_batch = pd.read_csv('streaming_batch.csv', sep='\\t', names=['user_id'], engine='c')\n",
    "    user_feature = pd.read_csv('user_feature.csv', sep='\\t', header=0, index_col=0, engine='c')\n",
    "    actions_id = list(pd.read_csv('actions.csv', sep='\\t', header=0, engine='c')['movie_id'])\n",
    "    reward_list = pd.read_csv('reward_list.csv', sep='\\t', header=0, engine='c')\n",
    "    action_context = pd.read_csv('action_context.csv', sep='\\t', header=0, engine='c')\n",
    "\n",
    "#     actions = []\n",
    "#     for key in actions_id:\n",
    "#         action = Action(key)\n",
    "#         actions.append(action)\n",
    "    actions = actions_id\n",
    "    return streaming_batch, user_feature, actions, reward_list, action_context\n",
    "\n",
    "\n",
    "def train_expert(action_context):\n",
    "    logreg = OneVsRestClassifier(LogisticRegression())\n",
    "    mnb = OneVsRestClassifier(MultinomialNB(), )\n",
    "    logreg.fit(action_context.iloc[:, 2:], action_context.iloc[:, 1])\n",
    "    mnb.fit(action_context.iloc[:, 2:], action_context.iloc[:, 1])\n",
    "    return [logreg, mnb]\n",
    "\n",
    "\n",
    "def get_advice(context, actions_id, experts):\n",
    "    advice = {}\n",
    "    for time in context.keys():\n",
    "        advice[time] = {}\n",
    "        for i in range(len(experts)):\n",
    "            prob = experts[i].predict_proba(context[time])[0]\n",
    "            advice[time][i] = {}\n",
    "            for j in range(len(prob)):\n",
    "                advice[time][i][actions_id[j]] = prob[j]\n",
    "    return advice\n",
    "\n",
    "\n",
    "def policy_generation(bandit, actions):\n",
    "    historystorage = history.MemoryHistoryStorage()\n",
    "    modelstorage = model.MemoryModelStorage()\n",
    "\n",
    "    if bandit == 'Exp4P':\n",
    "        policy = exp4p.Exp4P(actions, historystorage, modelstorage, delta=0.5, pmin=None)\n",
    "\n",
    "    elif bandit == 'LinUCB':\n",
    "#         policy = linucb.LinUCB(actions, historystorage, modelstorage, 0.3, 20)\n",
    "        policy = linucb.LinUCB(historystorage, modelstorage, actions, 0.3, 20)\n",
    "\n",
    "    elif bandit == 'LinThompSamp':\n",
    "#         policy = linthompsamp.LinThompSamp(historystorage, modelstorage, actions, d=20, delta=0.61, R=0.01, epsilon=0.71)\n",
    "        policy = linthompsamp.LinThompSamp(historystorage, modelstorage, actions, delta=0.61, R=0.01, epsilon=0.71)\n",
    "\n",
    "    elif bandit == 'UCB1':\n",
    "        policy = ucb1.UCB1(historystorage, modelstorage, actions)\n",
    "\n",
    "    elif bandit == 'Exp3':\n",
    "        policy = exp3.Exp3(historystorage, modelstorage, actions, gamma=0.2)\n",
    "\n",
    "    elif bandit == 'random':\n",
    "        policy = 0\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_evaluation(policy, bandit, streaming_batch, user_feature, reward_list, actions, action_context=None):\n",
    "    times = len(streaming_batch)\n",
    "    seq_error = np.zeros(shape=(times, 1))\n",
    "#     actions_id = [actions[i].action_id for i in range(len(actions))]\n",
    "    actions_id = [actions[i] for i in range(len(actions))]\n",
    "    if bandit in ['LinUCB', 'LinThompSamp', 'UCB1', 'Exp3']:\n",
    "        for t in range(times):\n",
    "            feature = np.array(user_feature[user_feature.index == streaming_batch.iloc[t, 0]])[0]\n",
    "            full_context = {}\n",
    "            for action_id in actions_id:\n",
    "                full_context[action_id] = feature\n",
    "            history_id, action = policy.get_action(full_context, 1)\n",
    "            watched_list = reward_list[reward_list['user_id'] == streaming_batch.iloc[t, 0]]\n",
    "\n",
    "            if action[0]['action'].action_id not in list(watched_list['movie_id']):\n",
    "                policy.reward(history_id, {action[0]['action'].action_id: 0.0})\n",
    "                if t == 0:\n",
    "                    seq_error[t] = 1.0\n",
    "                else:\n",
    "                    seq_error[t] = seq_error[t - 1] + 1.0\n",
    "\n",
    "            else:\n",
    "                policy.reward(history_id, {action[0]['action'].action_id: 1.0})\n",
    "                if t > 0:\n",
    "                    seq_error[t] = seq_error[t - 1]\n",
    "\n",
    "    elif bandit == 'Exp4P':\n",
    "        for t in range(times):\n",
    "            feature = user_feature[user_feature.index == streaming_batch.iloc[t, 0]]\n",
    "            experts = train_expert(action_context)\n",
    "            advice = {}\n",
    "            for i in range(len(experts)):\n",
    "                prob = experts[i].predict_proba(feature)[0]\n",
    "                advice[i] = {}\n",
    "                for j in range(len(prob)):\n",
    "                    advice[i][actions_id[j]] = prob[j]\n",
    "            history_id, action = policy.get_action(advice)\n",
    "            watched_list = reward_list[reward_list['user_id'] == streaming_batch.iloc[t, 0]]\n",
    "\n",
    "            if action[0]['action'].action_id not in list(watched_list['movie_id']):\n",
    "                policy.reward(history_id, {action[0]['action'].action_id: 0.0})\n",
    "                if t == 0:\n",
    "                    seq_error[t] = 1.0\n",
    "                else:\n",
    "                    seq_error[t] = seq_error[t - 1] + 1.0\n",
    "\n",
    "            else:\n",
    "                policy.reward(history_id, {action[0]['action'].action_id: 1.0})\n",
    "                if t > 0:\n",
    "                    seq_error[t] = seq_error[t - 1]\n",
    "\n",
    "    elif bandit == 'random':\n",
    "        for t in range(times):\n",
    "            action = actions_id[np.random.randint(0, len(actions)-1)]\n",
    "            watched_list = reward_list[reward_list['user_id'] == streaming_batch.iloc[t, 0]]\n",
    "\n",
    "            if action not in list(watched_list['movie_id']):\n",
    "                if t == 0:\n",
    "                    seq_error[t] = 1.0\n",
    "                else:\n",
    "                    seq_error[t] = seq_error[t - 1] + 1.0\n",
    "\n",
    "            else:\n",
    "                if t > 0:\n",
    "                    seq_error[t] = seq_error[t - 1]\n",
    "\n",
    "    return seq_error\n",
    "\n",
    "\n",
    "def regret_calculation(seq_error):\n",
    "    t = len(seq_error)\n",
    "    regret = [x / y for x, y in zip(seq_error, range(1, t + 1))]\n",
    "    return regret\n",
    "\n",
    "\n",
    "def main():\n",
    "    streaming_batch, user_feature, actions, reward_list, action_context = get_data()\n",
    "    streaming_batch_small = streaming_batch.iloc[0:10000]\n",
    "\n",
    "    # conduct regret analyses\n",
    "    experiment_bandit = ['LinUCB', 'LinThompSamp', 'Exp4P', 'UCB1', 'Exp3', 'random']\n",
    "#     experiment_bandit = 'Exp4P'\n",
    "    regret = {}\n",
    "    col = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    i = 0\n",
    "    for bandit in experiment_bandit:\n",
    "        policy = policy_generation(bandit, actions)\n",
    "        seq_error = policy_evaluation(policy, bandit, streaming_batch_small, user_feature, reward_list,\n",
    "                                      actions, action_context)\n",
    "        regret[bandit] = regret_calculation(seq_error)\n",
    "        plt.plot(range(len(streaming_batch_small)), regret[bandit], c=col[i], ls='-', label=bandit)\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('regret')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        axes = plt.gca()\n",
    "        axes.set_ylim([0, 1])\n",
    "        plt.title(\"Regret Bound with respect to T\")\n",
    "        i += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
