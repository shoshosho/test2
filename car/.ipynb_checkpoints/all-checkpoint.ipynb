{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import dask.dataframe as dskdf\n",
    "import dask\n",
    "gl = globals()\n",
    "start = time.time()\n",
    "\n",
    "def taken_time(word):\n",
    "    taken = time.time() - start\n",
    "    h = int(taken/60/60)\n",
    "    m = int(taken/60%60)\n",
    "    s = int(taken%60%60)\n",
    "    print(\"it takes \"+str(h)+\"h\"+str(m)+\"m\"+str(s)+\"s to finish \"+word)\n",
    "    \n",
    "class rawdata:\n",
    "    #地域系は全部で４つのデータをマージする\n",
    "    #車種系は全部で●つのデーたをマージする\n",
    "    #車種の元ネタが成約データなので、最終的に車種データに地域データを市区町村名で紐付ける\n",
    "\n",
    "    #地域(1)\n",
    "    #市区町村別国勢調査\n",
    "    def make_kokusei(self):\n",
    "        self.kokusei = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/kokuseichosa_2015_for_merge.csv\")\n",
    "        #全てを英字変換する必要あり\n",
    "        self.kokusei.columns=[\"pref_code\", \"pref_city_code\", \"capital_city_flg\", \"city_categories\", \"chiho\", \"city_name\",\n",
    "                         \"population\", \"h22_kumikae_population\", \"population_changed_from_h22_to_h27\", \"rate_population_changed_from_h22_to_h27\",\n",
    "                         \"gross_area\", \"population_density\", \"ave_age_from_h22_to_h27\", \"med_age_from_h22_to_h27\", \"population\",\n",
    "                         \"population_under_15old\", \"population_from_15_to_64_old\", \"population_over_65old\", \"rate_population_under_15_year_old\",\n",
    "                         \"rate_population_from_15_to_64_old\", \"rate_population_over_65old\", \"population_male\",\"population_male_under_15old\",\n",
    "                         \"population_male_from_15_to_64_old\", \"population_male_over_65old\", \"rate_population_male_under_15old\",\n",
    "                         \"rate_population_male_from_15_to_64_old\", \"rate_population_male_over_65old\", \"population_female\",\n",
    "                         \"population_female_under_15old\", \"population_female_from_15_to_64_old\", \"population_female_over_65old\",\n",
    "                         \"rate_population_female_under_15old\", \"rate_population_female_from_15_to_64_old\", \"rate_population_female_over_65old\",\n",
    "                         \"rate_population_male_to female\", \"population_jap\", \"population_no_jap\", \"num_of_family\", \"num_of_normal_family\",\n",
    "                         \"num_of_family_in_facilities\", \"h22_kumikae_family\", \"num_of_normal_family\", \"num_of_nuclear_family\",\n",
    "                         \"num_of_family_no_children\", \"num_of_family_with_children\", \"num_of_father_and_children\", \"num_of_mother_and_children\",\n",
    "                         \"num_of_singles\", \"num_of_over_65old_singles\", \"num_of_old_married_couples_no_children\", \"num_of_3generation_family\",]\n",
    "        return self.kokusei\n",
    "\n",
    "    #chiho(都道府県名)で国勢調査と保有台数をマージ\n",
    "    def left_merge_on_chiho(self,left,right,out):\n",
    "        out = pd.merge(left,right,on=\"chiho\",how=\"left\")\n",
    "        #一番下に謎の欠損行が存在するので削除する\n",
    "        out = out.dropna(subset=['pref_code'])\n",
    "        return out\n",
    "\n",
    "    #地域(2)\n",
    "    #都道府県別保有台数\n",
    "    def make_hoyu(self):\n",
    "        hoyu = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/2015_hoyu_daisu.csv\",encoding=\"utf-8\")\n",
    "        hoyu.columns = [\"chiho\", \"num_of_cars_all\", \"num_of_cars_kei\", \"num_of_cars_normal\", \"num_of_familiy\", \"cars_per_person\",\n",
    "                        \"normal_cars_per_person\", \"kei_cars_per_person\", \"kei_rate\", \"normal_rate\", \"kei_share_in_JP\", \"normal_share_in_JP\",\n",
    "                        \"guess_2nd_hand_kei_car_sales_per_year\", \"guess_2nd_hand_normal_car_sales_per_year\", \"guess_2nd_hand_car_all_sales_per_year\"]\n",
    "        self.koku_hoyu = self.left_merge_on_chiho(self.kokusei, hoyu, \"self.koku_hoyu\")\n",
    "        return self.koku_hoyu\n",
    "\n",
    "\n",
    "    #car_sensor在庫/見積問い合わせの大元データ作成\n",
    "    def make_toiawase(self):\n",
    "        # self.kuruma = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/物件*問い合わせ/tes.csv\")\n",
    "        ku = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/物件*問い合わせ/toiawase_1.csv\",\n",
    "                         delimiter=\"\\t\", dtype=\"object\", encoding=\"cp932\")\n",
    "        ru = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/物件*問い合わせ/toiawase_2.csv\",\n",
    "                         delimiter=\"\\t\", dtype=\"object\", encoding=\"cp932\",names=ku.columns)\n",
    "        ma = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/物件*問い合わせ/toiawase_3.csv\",\n",
    "                         delimiter=\"\\t\", dtype=\"object\", encoding=\"cp932\",names=ku.columns)\n",
    "        kuru = pd.concat([ku,ru],axis=0)\n",
    "        self.kuruma = pd.concat([kuru,ma],axis=0)\n",
    "        #表記の統一\n",
    "        self.kuruma[\"b.shikuchoson_jusho_kj\"] = self.kuruma[\"b.shikuchoson_jusho_kj\"].str.replace('ヶ', 'ケ')\n",
    "        #前後スペースの削除\n",
    "        self.kuruma[\"b.todofuken_jusho_kj\"] = self.kuruma[\"b.todofuken_jusho_kj\"].map(lambda x: str(x).strip())\n",
    "        # 都道府県欠損を削除\n",
    "        self.kuruma = self.kuruma.dropna(subset=[\"b.todofuken_jusho_kj\"])\n",
    "        # 都道府県名に異常値が入っているものを除く\n",
    "        # '〓','null','nan'が入っていたら削除\n",
    "        self.kuruma = self.kuruma.ix[self.kuruma['b.todofuken_jusho_kj'] != \"〓\"]\n",
    "        self.kuruma = self.kuruma.ix[self.kuruma['b.todofuken_jusho_kj'] != \"null\"]\n",
    "        self.kuruma = self.kuruma.ix[self.kuruma['b.todofuken_jusho_kj'] != \"nan\"]\n",
    "\n",
    "        #県単位、市区町村単位での地方別集計（問い合わせ）\n",
    "        #もともとの市区町村名に入ってるレベル感が違う(市区町村で終わるものもあれば、町大字まで入っているものもある)\n",
    "        #市区町村マスタと一致する部分のみ残す\n",
    "        dl = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/市区町村マスタ.csv\",names=[\"pref\",\"city\"])\n",
    "\n",
    "        #h市区町村表記揺れを消したい\n",
    "        dl[\"city\"] = dl[\"city\"].str.replace('ヶ', 'ケ')\n",
    "        pre = dl[\"pref\"].unique() #array\n",
    "        #県内市区町村数のカウント用\n",
    "        dl[\"count\"] =1\n",
    "        num = dl.groupby(\"pref\").sum()\n",
    "        dc = num.to_dict()\n",
    "        dic = dc[\"count\"]\n",
    "\n",
    "        biglis = []\n",
    "        c=0\n",
    "        #都道府県別市区町村の格納\n",
    "        for i in pre:\n",
    "            tmplis = []\n",
    "            for k in range(int(dic[i])):\n",
    "        #             exec(\"tmplis.append(dl['city'][\"+str(c)+\"])\")\n",
    "                tmplis.append(dl['city'][c])\n",
    "                c +=1\n",
    "            #元データの入り方が札幌市→札幌市〇〇区、といった順で入っているので、上から部分一致で合致を探す際の障害になる\n",
    "            #ゆえに最初の要素を最後に持ってくる処理が必要\n",
    "            push_to_last = tmplis.pop(0)\n",
    "            tmplis.append(push_to_last)\n",
    "            biglis.append(tmplis)\n",
    "\n",
    "        #辞書の作成\n",
    "        #県名をキーとして、市区町村名のリストをバリューとする\n",
    "        dicts = {key: value for (key, value) in zip(pre,biglis)}\n",
    "\n",
    "        #dfの各要素について、辞書のどこに一致するかを見て、ループで部分一致を探す\n",
    "        #部分一致したものをtoshiで保持する\n",
    "        ken = self.kuruma[\"b.todofuken_jusho_kj\"].values\n",
    "        org_toshi = self.kuruma[\"b.shikuchoson_jusho_kj\"].values\n",
    "        toshi = [] #整形した市区町村名\n",
    "        counter =[] #部分一致するものがあるかどうかのフラグ\n",
    "        for k,i in zip(ken,org_toshi):\n",
    "            count=0\n",
    "            for l in dicts[k]:\n",
    "                if l in i:\n",
    "                    toshi.append(l)\n",
    "                    counter.append(1)\n",
    "                    break\n",
    "                count+=1\n",
    "                if count >=(len(dicts[k])):\n",
    "                    toshi.append(\"\")\n",
    "                    counter.append(0)\n",
    "                    break\n",
    "        #概ねOKだけど、県名の後に直接郡の名前が来るケースに対応できていない\n",
    "        # →修正面倒なので後回し\n",
    "        #スクレイピングで持ってくる必要ある：　http://www.geocities.jp/je2kcr/current_allgun.htm\n",
    "        #toshiが欠損のものはとりあえず除外検討\n",
    "\n",
    "        #bbをkurumaにマージ\n",
    "        self.kuruma[\"city_name\"] = toshi\n",
    "        self.kuruma[\"count\"] = 1\n",
    "        #toiawase_dateの型を日付型に変更する必要がある\n",
    "        self.kuruma[\"toiawase_datetime\"] = pd.to_datetime(self.kuruma[\"b.toiawase_date\"].str[:10] + \" \" + self.kuruma[\"b.toiawase_date\"].str[10:])\n",
    "        self.kuruma[\"toiawase_y\"] = self.kuruma[\"toiawase_datetime\"].dt.year.astype(str)\n",
    "        self.kuruma[\"toiawase_m\"] = self.kuruma[\"toiawase_datetime\"].dt.month.astype(str)\n",
    "        self.kuruma[\"toiawase_ym\"] = self.kuruma[\"toiawase_y\"] + self.kuruma[\"toiawase_m\"]\n",
    "        return self.kuruma\n",
    "\n",
    "    # 地域別集計(問い合わせ)\n",
    "    # 全て辞書形式で持っておいて、実際の車に紐付ける形にする\n",
    "    # 実際の元データの車種名、地域、スペックと紐付けたドア数をキーにして辞書から値を作成する\n",
    "    def convert_to_unit_based_num(self, data, colname, indice):\n",
    "        out = data.reset_index().pivot(index=indice, columns=colname, values=\"count\")\n",
    "        vals = []\n",
    "        # 異常値の除去\n",
    "        for i in out.columns:\n",
    "            vals.append(out.columns.name + \"_\" + str(i))\n",
    "        out.columns = vals\n",
    "        out[indice] = out.index\n",
    "        del out.index.name\n",
    "\n",
    "        return out\n",
    "\n",
    "    #地域(3)\n",
    "    #問い合わせの地域別集計\n",
    "    def toiawase_shukei_area(self):\n",
    "        #都道府県\n",
    "        pref_summary = self.kuruma.groupby(\"b.todofuken_jusho_kj\").sum()[\"count\"]\n",
    "        pref_summary_dicts = {key: value for (key, value) in zip(pref_summary.index.values,pref_summary.values)}\n",
    "        self.kuruma[\"num_toiawase_in_pref\"] = self.kuruma[\"b.todofuken_jusho_kj\"].apply(pref_summary_dicts.get)\n",
    "\n",
    "        #市区町村\n",
    "        city_summary = self.kuruma.groupby(\"city_name\").sum()[\"count\"]\n",
    "        city_summary_dicts = {key: value for (key, value) in zip(city_summary.index.values,city_summary.values)}\n",
    "        self.kuruma[\"num_toiawase_in_city\"] = self.kuruma[\"city_name\"].apply(city_summary_dicts.get)\n",
    "\n",
    "        #都道府県/市区町村単位でボディタイプ、問合年月、ドア数で集計（計６パターン）\n",
    "        shukei_val = [\"a.body_type_name\", \"toiawase_ym\", \"a.door_su\"]\n",
    "        out_name = [\"body\", \"month\", \"door\",]\n",
    "\n",
    "        pref_city = [\"b.todofuken_jusho_kj\",\"city_name\"]\n",
    "        out_preposition = [\"pref\", \"city\"]\n",
    "\n",
    "        for s,o in zip(shukei_val,out_name):\n",
    "            for p,op in zip(pref_city,out_preposition):\n",
    "                su = str(op)+'_'+str(o)+'_summary' #中間df名\n",
    "                sud = str(op)+'_'+str(o)+'_summary_dicts' #辞書名\n",
    "                va = 'num_toiawase_'+str(o)+'_in_'+str(p) #変数名\n",
    "                df2 = o+'_by_'+op #最終df名\n",
    "\n",
    "                gl[su] = self.kuruma.groupby([p,s]).sum()[\"count\"]\n",
    "                gl[sud] = {key: value for key, value in zip(gl[su].index.values,gl[su].values)} #辞書に格納\n",
    "                self.kuruma[va] = self.kuruma.apply(lambda x :gl[sud].get((x[p],x[s])), 1) #変数を問い合わせに追加\n",
    "                gl[df2] = self.convert_to_unit_based_num(gl[su],s,p) #県別/市区町村別でテーブル作成\n",
    "\n",
    "        #出来上がったデータセットをすべてくっつける\n",
    "        cities = [body_by_city, door_by_city, month_by_city]\n",
    "        prefs = [body_by_pref, door_by_pref, month_by_pref]\n",
    "\n",
    "        #prefsの変数名を変更しておく必要がある\n",
    "        #遅いと思われるけど、シンプルさを優先した\n",
    "        for c in cities:\n",
    "            self.koku_hoyu = pd.merge(self.koku_hoyu,c,on=\"city_name\")\n",
    "        for p in prefs:\n",
    "            p = p.rename(columns={\"b.todofuken_jusho_kj\":\"chiho\"})\n",
    "            self.koku_hoyu = pd.merge(self.koku_hoyu,p,on=\"chiho\")\n",
    "\n",
    "        self.koku_hoyu_area = self.koku_hoyu.copy()\n",
    "        return self.koku_hoyu_area\n",
    "\n",
    "    #車種(1)\n",
    "    #車種・グレード別集計（問い合わせ）\n",
    "    def toiawase_shukei_shashu(self):\n",
    "        #body_typeごと\n",
    "        body_summary = self.kuruma.groupby(\"a.body_type_name\").sum()[\"count\"]\n",
    "        body_summary_dicts = {key: value for (key, value) in zip(body_summary.index.values,body_summary.values)}\n",
    "        self.kuruma[\"num_body_type\"] = self.kuruma[\"a.body_type_name\"].apply(body_summary_dicts.get)\n",
    "        # -車種ごと\n",
    "        shashu_summary = self.kuruma.groupby(\"a.keisai_shashu_kj\").sum()[\"count\"]\n",
    "        shashu_summary_dicts = {key: value for (key, value) in zip(shashu_summary.index.values,shashu_summary.values)}\n",
    "        self.kuruma[\"num_shashu\"] = self.kuruma[\"a.keisai_shashu_kj\"].apply(shashu_summary_dicts.get)\n",
    "        # -車種&FMCごと\n",
    "        shashu_fmc_summary = self.kuruma.groupby([\"a.keisai_shashu_kj\",\"a.keisai_fmc_cd\"]).sum()[\"count\"]\n",
    "        shashu_fmc_summary_dicts = {key: value for (key, value) in zip(shashu_fmc_summary.index.values,shashu_fmc_summary.values)}\n",
    "        self.kuruma[\"num_shashu_fmc\"] = self.kuruma.apply(lambda x :shashu_fmc_summary_dicts.get((x[\"a.keisai_shashu_kj\"],x[\"a.keisai_fmc_cd\"])), 1)\n",
    "        # fmc_by_shashu = self.convert_to_unit_based_num(shashu_fmc_summary,\"a.keisai_shashu_kj\",\"a.keisai_fmc_cd\")\n",
    "\n",
    "        #要修正＿済み\n",
    "        # -車種&FMC&MCごと\n",
    "        shashu_fmc_mc_summary = self.kuruma.groupby([\"a.keisai_shashu_kj\",\"a.keisai_fmc_cd\",\"a.keisai_mc_cd\"]).sum()[\"count\"]\n",
    "        shashu_fmc_mc_summary_dicts = {key: value for (key, value) in zip(shashu_fmc_mc_summary.index.values,shashu_fmc_mc_summary.values)}\n",
    "        self.kuruma[\"num_shashu_fmc_mc\"] = self.kuruma.apply(lambda x :shashu_fmc_mc_summary_dicts.get((x[\"a.keisai_shashu_kj\"],x[\"a.keisai_fmc_cd\"],x[\"a.keisai_mc_cd\"])), 1)\n",
    "        # fmc_mc_by_shashu = self.convert_to_unit_based_num(shashu_fmc_mc_summary,\"a.keisai_shashu_kj\",[\"a.keisai_fmc_cd\",\"a.keisai_mc_cd\"])\n",
    "        # -車種&FMC&MC&グレードごと\n",
    "        shashu_fmc_mc_grade_summary = self.kuruma.groupby([\"a.keisai_shashu_kj\",\"a.keisai_fmc_cd\",\"a.keisai_mc_cd\",\"a.keisai_haikiryo_grade_cd\"]).sum()[\"count\"]\n",
    "        shashu_fmc_mc_grade_summary_dicts = {key: value for (key, value) in zip(shashu_fmc_mc_grade_summary.index.values,shashu_fmc_mc_grade_summary.values)}\n",
    "        self.kuruma[\"num_shashu_fmc_mc_grade\"] = self.kuruma.apply(lambda x :shashu_fmc_mc_grade_summary_dicts.get((x[\"a.keisai_shashu_kj\"],x[\"a.keisai_fmc_cd\"],x[\"a.keisai_mc_cd\"],x[\"a.keisai_haikiryo_grade_cd\"])), 1)\n",
    "\n",
    "        #車種・グレード別集計（お気に入り）\n",
    "        #地域みたいにまとめることができないので、個別のDFを吐き出しておく\n",
    "        return [body_summary, shashu_summary, shashu_fmc_summary, shashu_fmc_mc_summary, shashu_fmc_mc_grade_summary]\n",
    "\n",
    "\n",
    "    #地域(4)\n",
    "    #カスタマDBの作成\n",
    "    def summarize_cusdb(self):\n",
    "        cus = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/cusdb_all.csv\")\n",
    "        # 予測含め、地域が入ってないものを捨てる\n",
    "        # 予測精度約７割程度はあるので、まあいいかなと\n",
    "        city_drop_na = cus.dropna(subset=['pref_integration_cd'])\n",
    "        # 実際のデータセットはpref_integration_nmで作成する\n",
    "\n",
    "\n",
    "        tmp_dt = city_drop_na.ix[:, [\"count\", \"have_child_integration_flg\",\n",
    "                                     \"pref_integration_nm\"]].groupby(\n",
    "            [\"pref_integration_nm\", \"have_child_integration_flg\"]).sum()\n",
    "        # 県別にフラグの値をカウント\n",
    "        merged = tmp_dt.reset_index().pivot(index=tmp_dt.index.names[0], columns=tmp_dt.index.names[1], values=\"count\")\n",
    "        lis = []\n",
    "        for i in merged.columns:\n",
    "            lis.append(str(merged.columns.name) + \"_\" + str(i))\n",
    "        merged.columns = lis\n",
    "        del merged.index.name\n",
    "        merged[\"pref\"] = merged.index\n",
    "        # merged.head()\n",
    "\n",
    "        # 変数を動的に与えて、ループでデータセットを作成＆append\n",
    "        # フラグ系対象変数\n",
    "        # sp系は数値が入っていないので、最初は抜いて試す\n",
    "        # iter_target = [\"sex_integration\", \"age_integration_cd\", \"high_income_predict_flg\",\n",
    "        #                \"businessperson_integration_flg\", \"couple_flg\", \"have_baby_flg\", \"car_1w_pv_flg\", \"car_1m_pv_flg\", \"car_3m_pv_flg\",\n",
    "        #                \"car_6m_pv_flg\", \"car_12m_pv_flg\", \"sp_holiday_flg\", \"sp_weekday_flg\", \"pc_holiday_flg\", \"pc_weekday_flg\", \"holiday_flg\",\n",
    "        #                \"weekday_flg\", \"sp_holiday_time_seg\", \"sp_weekday_time_seg\", \"pc_holiday_time_seg\", \"pc_weekday_time_seg\",\n",
    "        #                \"holiday_time_seg\", \"weekday_time_seg\", \"weekday_device_seg\"]\n",
    "        iter_target = [\"sex_integration\", \"age_integration_cd\", \"high_income_predict_flg\",\n",
    "                       \"businessperson_integration_flg\", \"couple_flg\", \"have_baby_flg\", \"car_1w_pv_flg\", \"car_1m_pv_flg\",\n",
    "                       \"car_3m_pv_flg\", \"car_6m_pv_flg\", \"car_12m_pv_flg\"]\n",
    "\n",
    "        loop_count = 0\n",
    "        for i in iter_target:\n",
    "            loop_count += 1\n",
    "            tmp_dt = city_drop_na.ix[:, [\"count\", str(i), \"pref_integration_nm\"]].groupby(\n",
    "                [\"pref_integration_nm\", str(i)]).sum()\n",
    "            #         exec('tmp_dt = city_drop_na.ix[:,[\"count\",\"'+str(i)+'\",\"pref_integration_nm\"]].groupby([\"pref_integration_nm\",\"'+str(i)+'\"]).sum()')\n",
    "            # 県別にフラグの値をカウント\n",
    "            tmp = tmp_dt.reset_index().pivot(index=tmp_dt.index.names[0], columns=tmp_dt.index.names[1], values=\"count\")\n",
    "            tmp_rate = tmp.copy() / len(city_drop_na)\n",
    "            # 一時対応策：カラム名が同一になってしまい、後で不都合が起こるので、一時的に適当なカラム名を与える\n",
    "            tmp_lis = (np.array(list(range(len(tmp_rate.T)))) + 1) * 10\n",
    "            tmp_rate.columns = tmp_lis\n",
    "\n",
    "            lis = []\n",
    "            for n in range(2):\n",
    "                for i in tmp.columns:\n",
    "                    if n == 0:\n",
    "                        lis.append(str(tmp.columns.name) + \"_\" + str(i))\n",
    "                    if n == 1:\n",
    "                        lis.append(str(tmp.columns.name) + \"_\" + str(i) + \"_rate\")\n",
    "\n",
    "            tmp = pd.concat([tmp, tmp_rate], axis=1)\n",
    "            tmp.columns = lis\n",
    "            del tmp.index.name\n",
    "            tmp[\"pref\"] = tmp.index\n",
    "            # 率も入れる\n",
    "\n",
    "            # できたものを元のデータセットにマージしていく\n",
    "            merged = pd.merge(merged, tmp, on=\"pref\")\n",
    "\n",
    "        # 数値系の統合(年収、年齢：平均、中央値)\n",
    "        num_loop = [\"annual_income_predict_value\", \"annual_income_integration_cd\", \"age_integration_value\"]\n",
    "\n",
    "        for l in num_loop:\n",
    "            tmp_mean = city_drop_na.ix[:, [str(l), \"pref_integration_nm\"]].groupby([\"pref_integration_nm\"]).mean()\n",
    "            tmp_med = city_drop_na.ix[:, [str(l), \"pref_integration_nm\"]].groupby([\"pref_integration_nm\"]).median()\n",
    "            tmp_mean.columns = [str(l) + \"_mean\"]\n",
    "            tmp_med.columns = [str(l) + \"_med\"]\n",
    "            tmp_mean[\"pref\"] = tmp_mean.index\n",
    "            tmp_med[\"pref\"] = tmp_med.index\n",
    "            merged = pd.merge(merged, tmp_mean, on=\"pref\")\n",
    "            merged = pd.merge(merged, tmp_med, on=\"pref\")\n",
    "\n",
    "            cus_extracted = merged.copy()\n",
    "            cus_extracted = cus_extracted.rename(columns={\"pref\": \"chiho\"})\n",
    "\n",
    "            # 最後に市区町村単位にサマッたカスタマDBを民力にマージする\n",
    "            self.koku_hoyu_area_cusdb = pd.merge(self.koku_hoyu_area, cus_extracted, on=\"chiho\")\n",
    "\n",
    "            return self.koku_hoyu_area_cusdb\n",
    "\n",
    "\n",
    "\n",
    "    #大元データ\n",
    "    #お気に入り＆成約データ読み込み\n",
    "    #お気に入りデータだけだと成立しないので、先んじて成約データとのマージをしておく必要がある\n",
    "    def make_seiyaku(self):\n",
    "        #成約データ\n",
    "        #要修正\n",
    "        #本番はdaskで読む必要あると思われるのでそうする\n",
    "        self.seiyaku = dskdf.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/org_seiyaku.csv\",delimiter=\" \")\n",
    "\n",
    "        #きちんとテストする際に全量で実施する必要あり\n",
    "\n",
    "        #お気に入りデータ\n",
    "        col = [\"wns_bukken_cd\", \"shinkoyo_nohimbi_cd\", \"han_cd\", \"hojin_cd\", \"madoguchi_cd\", \"sum_toiawase\",\n",
    "               \"base_plan_toiawase\", \"plan1_toiawase\", \"plan2_toiawase\", \"cart_add\", \"cart_delete\"]\n",
    "        #dask使わないと遅すぎて無理\n",
    "        start = time.time()\n",
    "        tes1 = dskdf.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/favorite/001.csv\",names=col)\n",
    "        tes2 = dskdf.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/favorite/002.csv\",names=col)\n",
    "        tes3 = dskdf.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/favorite/003.csv\",names=col)\n",
    "        tes4 = dskdf.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/favorite/004.csv\",names=col)\n",
    "        tes5 = dskdf.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/favorite/005.csv\",names=col)\n",
    "\n",
    "        t1 = tes1.append(tes2)\n",
    "        t2 = t1.append(tes3)\n",
    "        t3 = t2.append(tes4)\n",
    "        t4 = t3.append(tes5)\n",
    "\n",
    "        fav = t4.loc[:,[\"wns_bukken_cd\", \"shinkoyo_nohimbi_cd\", \"sum_toiawase\", \"cart_add\", \"cart_delete\"]]\n",
    "\n",
    "        # データが多すぎて先に車種単位にまとめないと処理不可能\n",
    "        #物件tblからデータを抜いてきて（wns_bukken_cd,車種名、車種に関連するものだけ）突合、groupbyで車種ごとにサマるところからスタート\n",
    "        tobe = self.seiyaku.ix[:,[\"wns_bukken_cd\", \"shashu\", \"grade_cd\", \"grade_kukuri_cd\", \"body_type_cd\", \"body_type_name\"]]\n",
    "        #rightはpandas DataFrameじゃないと何故か上手くいかない・・・\n",
    "        self.bukken = fav.merge(tobe,how=\"inner\",on=\"wns_bukken_cd\")\n",
    "        self.bukken[\"count\"]=1\n",
    "\n",
    "        return [self.seiyaku, self.bukken]\n",
    "\n",
    "    #車種(2)\n",
    "    #車種別全期間(直近３年間)お気に入り数集計\n",
    "    def okini_shukei_zenkikan(self):\n",
    "        #全体のお気に入り数を出す\n",
    "        zentai = self.bukken.groupby([\"shashu\",\"grade_cd\"]).sum()\n",
    "        self.okini_zen = zentai.reset_index()\n",
    "        self.okini_zen[\"shinkoyo_nohimbi_cd\"] = \"all\"\n",
    "        self.okini_zen[\"cart_sum\"] = self.okini_zen[\"cart_add\"] - self.okini_zen[\"cart_delete\"]\n",
    "\n",
    "        return self.okini_zen\n",
    "\n",
    "    #車種(2.5)\n",
    "    #車種別期間別(特定の１年間)お気に入り数集計\n",
    "    # 死ぬほど時間かかるので、今回は飛ばしでいいかも\n",
    "    def okini_shukei_kikanbetsu(self):\n",
    "        # 期間を絞って（１年分、１ヶ月単位）お気に入り数の遷移を見る\n",
    "        # 最終的には車種、グレード、nohimbiをキーとした辞書を作成し、車種ごとのお気に入り数を当てる感じになる\n",
    "        # shashuとgrade_cdでgroupbyする\n",
    "        sam = self.bukken.groupby([\"shashu\", \"grade_cd\", \"shinkoyo_nohimbi_cd\"]).sum()\n",
    "        # 元テーブルとして使う\n",
    "        samr = sam.reset_index()\n",
    "        # shinkoyo_nohinbiが特定日付区間に入っているものだけを抜き出して、groupby(shashu,grade_cd)で処理する\n",
    "        upper_date = 151030\n",
    "        # upper_dateが100ずつ足されていく感じ\n",
    "        # dataframeに入ってる最終日を超えたら処理中断\n",
    "        # limit = int(t5[\"shinkoyo_nohimbi_cd\"][len(t5)-1])\n",
    "        limit = int(t5[\"shinkoyo_nohimbi_cd\"].tail().iloc[4])\n",
    "        loop_count = 0\n",
    "\n",
    "        while upper_date <= limit:\n",
    "            lower = samr[samr[\"shinkoyo_nohimbi_cd\"] > (upper_date - 10000)]\n",
    "            lowup = lower[lower[\"shinkoyo_nohimbi_cd\"] <= upper_date]\n",
    "\n",
    "            # 日付を絞ってループで回す\n",
    "            if len(lowup) > 0:\n",
    "                smpl = lowup\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # 実際の集計\n",
    "            part = smpl.groupby([\"shashu\", \"grade_cd\"]).sum()\n",
    "            datum = part.reset_index()\n",
    "            # 変数名の加工\n",
    "\n",
    "            dt = str(smpl[\"shinkoyo_nohimbi_cd\"].tail().iloc[4])\n",
    "            col_list = [\"sum_toiawase\", \"cart_add\", \"cart_delete\", \"count\"]\n",
    "            new_col = [\"shashu\", \"grade_cd\", \"shinkoyo_nohimbi_cd\"]\n",
    "            for k in col_list:\n",
    "                new_col.append(\"from_\" + str(dt) + \"_1y_back_\" + k)\n",
    "\n",
    "            datum.columns = new_col\n",
    "            datum = datum.drop(\"shinkoyo_nohimbi_cd\", axis=1)\n",
    "            datum = datum.drop(\"count\", axis=1)\n",
    "            datum[\"favorite\"] = datum[\"cart_add\"] - datum[\"cart_delete\"]\n",
    "            upper_date += 100\n",
    "            if int(upper_date / 100 % 100) == 13:\n",
    "                upper_date = upper_date + 8700  # 年を更新する処理\n",
    "            # マージでなくて、shinkoyo_nohimbi_cdの違うものが縦に積みあがるケースも考慮すべき\n",
    "\n",
    "            if loop_count != 0:\n",
    "                #         okini.merge(datum,on=[\"shashu\",\"grade_cd\"])\n",
    "                self.okini_kikan.merge(datum, how=\"outer\", on=[\"shashu\", \"grade_cd\"], )\n",
    "            else:\n",
    "                self.okini_kikan = datum\n",
    "            loop_count += 1\n",
    "\n",
    "        # if loop_count >2:\n",
    "        #         break\n",
    "\n",
    "        # あとはこれをshashu,grade_cdで横に繋げていく(merge)\n",
    "        # 一番重い処理なので、回さないほうがいいかも\n",
    "        return self.okini_kikan\n",
    "\n",
    "\n",
    "    #車種(3)\n",
    "    #カタログデータ（車種別/モデル別スペック表）の作成\n",
    "    # 全部で21種あるので、全て読み込んだ後にカタログデータを１つにまとめる\n",
    "    #データ量が非常に大きいので、daskで処理させる必要あり\n",
    "\n",
    "    def make_catalogue(self):\n",
    "        cat_list = [\"mc1_body_type.txt\", \"mc1_kanren_shashu.txt\", \"mc1_brand.txt\", \"mc1_kihon_iro.txt\",\n",
    "                    \"mc1_brand_country.txt\",\n",
    "                    \"mc1_kukuri_kanren_shashu.txt\", \"mc1_brand_iro.txt\", \"mc1_maker.txt\", \"mc1_brand_maker.txt\",\n",
    "                    \"mc1_mc_model.txt\",\n",
    "                    \"mc1_country.txt\", \"mc1_mc_model_gazo.txt\", \"mc1_fmc_model.txt\", \"mc1_net_category.txt\",\n",
    "                    \"mc1_fp_shashu_midashi_settei_work.txt\", \"mc1_shashu.txt\", \"mc1_grade.txt\", \"mc1_sobi.txt\",\n",
    "                    \"mc1_grade_iro.txt\",\n",
    "                    \"mc1_transmission_henkan.txt\", \"mc1_grade_sobi.txt\"]\n",
    "        index_name = [\"ボディ形状マスタ\", \"関連車種マスタ\", \"ブランドマスタ\", \"基本色マスタ\", \"ブランド国設定マスタ\", \"括り関連車種マスタ\",\n",
    "                      \"ブランド色マスタ\", \"メーカーマスタ\", \"ブランドメーカー設定マスタ\", \"MCモデルマスタ\", \"国マスタ\", \"MCモデル画像マスタ\",\n",
    "                      \"FMCモデルマスタ\", \"NETカテゴリマスタ\", \"FP車種見出し設定マスタ\", \"車種マスタ\", \"グレードマスタ\", \"装備マスタ\",\n",
    "                      \"グレード色設定マスタ\", \"トランスミッション変換マスタ\", \"グレード装備設定マスタ\", ]\n",
    "        df_columns = [[\"body_shape_cd\", \"body_shape_nm\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"kanren_shashu_cate\", \"kanren_shashu_cate_no\", \"kanren_kj\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"brand_nm\", \"brand_hyoji_order\", \"catalogue_keisai_fuka_cate\", \"seo_folder_nm\",\n",
    "                       \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"base_color_cd\", \"base_color_nm\", \"base_color_hyoji_jun\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"country_cd\", \"daihyo_country_flg\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"kanren_shashu_cate\", \"kanren_shashu_cate_no\", \"brand_cd\", \"shashu_cd\",\n",
    "                       \"kukuri_kanren_shashu_hyoji_jun\",\n",
    "                       \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"brand_color_cd\", \"brand_color_nm\", \"base_color_cd\", \"jitsu_brand_color_nm\",\n",
    "                       \"brand_color_hyoji_order\", \"base_color_cd1\", \"base_color_cd2\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"maker_cd\", \"maker_nm\", \"maker_hyoji_jun\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"maker_cd\", \"daihyo_maker_flg\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"mc_hanbai_start\", \"mc_hanbai_end\", \"mc_cate\", \"mc_kj\",\n",
    "                       \"mc_commment_title\", \"mc_comment\", \"catalogue_keisai_fuka_cate\", \"register_ymd\",\n",
    "                       \"last_modified_ymd\"],\n",
    "                      [\"country_cd\", \"country_nm\", \"area_cate\", \"country_hyoji_jun\", \"register_ymd\", \"daihyo_base_color\",\n",
    "                       \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"gazo_index_no\", \"mc_model_pic_hyoji_jun\",\n",
    "                       \"gazo_cate_cd\",\n",
    "                       \"caption\", \"catalogue_keisai_fuka_cate\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"fmc_hanbai_start\", \"fmc_hanbai_end\", \"fmc_kj\",\n",
    "                       \"catalogue_keisai_fuka_cate\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"net_cate_cd\", \"cate_nm\", \"net_cate_hyoji_jun\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\", \"fp_cate_cd\", \"fp_shashu_midashi_cd\",\n",
    "                       \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"shashu_nm\", \"wareki_seireki_cate\", \"shashu_hyoji_jun\",\n",
    "                       \"catalogue_keisai_fuka_cate\", \"search_key_1\", \"search_key_2\", \"search_key_3\", \"seo_folder_nm\",\n",
    "                       \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\", \"grade_kukuri_cd\", \"grade_nm\",\n",
    "                       \"grade_hyoji_jun\",\n",
    "                       \"body_shape_cd\", \"grade_hanbai_start_ym\", \"grade_hanbai_end_ym\", \"katashiki\", \"ruibetsu_kigo\",\n",
    "                       \"katashiki_shitei_no\", \"ruibetsu_cate_no\", \"price_with_tax\", \"price_without_tax\",\n",
    "                       \"nenpi_kijun_achieve_cate\", \"nenpi_kijun_nen\", \"low_emission_nintei_stars\",\n",
    "                       \"low_emission_nintei_nen\",\n",
    "                       \"reduce_tax_cate\", \"heiko_yunyusha_cate\", \"walfare_car_cate\", \"car_for_commercial_use\", \"kei_flg\",\n",
    "                       \"number_size\", \"special_edition_flg\", \"catalogue_keisai_fuka_cate\", \"keisai_commentary\",\n",
    "                       \"kudo_hoshiki_cate\", \"transmission_su\", \"transmission_cate\", \"transmission_ichi_cate\",\n",
    "                       \"transmission_cd\",\n",
    "                       \"handle_ichi_cate\", \"length\", \"width\", \"height\", \"inner_length\", \"inner_width\", \"inner_height\",\n",
    "                       \"wheel_base\", \"front_tred\", \"back_tred\", \"min_shako\", \"weight_of_car\", \"weight_of_car_all\",\n",
    "                       \"max_carriage\", \"max_carriage_persons\", \"min_kaiten_hankei\", \"10_15_mode_nenpiritsu\",\n",
    "                       \"stearing_gear_cate\", \"stearing_gear_nm\", \"front_suspension_cate\", \"front_suspension_nm\",\n",
    "                       \"back_suspension_cate\", \"back_suspension_nm\", \"front_break_cate\", \"front_break_nm\",\n",
    "                       \"back_break_cate\",\n",
    "                       \"back_break_nm\", \"front_tire_txt\", \"back_tire_txt\", \"engine_katashiki\", \"engine_valve_cate\",\n",
    "                       \"engine_silinder_cate\", \"engine_kito_su\", \"engine_kakyuki_cate\", \"engine_cate\", \"silinder_radius\",\n",
    "                       \"silinder_koutei\", \"total_haikiryo\", \"haikiryo1\", \"haikiryo2\", \"asshukuhi\", \"max_shuturyoku_kw\",\n",
    "                       \"max_shuturyoku_ps\", \"max_shuturyoku_rmp_max\", \"max_shuturyoku_rmp_min\", \"max_torque_mn\",\n",
    "                       \"max_torque_kgm\", \"max_torque_rpm_max\", \"max_torque_rpm_min\", \"fuel_sapply_device_cate\", \"fuel_cate\",\n",
    "                       \"fuel_tank_capacity\", \"door_su\", \"seat_columns\", \"net_cate_cd\", \"register_ymd\", \"last_modified_ymd\",\n",
    "                       \"jc08_mode_nenpi_rtsu\"],\n",
    "                      [\"equipment_cd\", \"equipment_nm\", \"equipment_bunrui_cate\", \"catalogue_keisai_cate\",\n",
    "                       \"equipment_hyoji_jun\",\n",
    "                       \"equipment_usage_cd\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\", \"brand_color_cd\", \"option_flg\",\n",
    "                       \"option_price\",\n",
    "                       \"grade_hyoji_jun\", \"register_ymd\", \"last_modified_ymd\"],\n",
    "                      [\"transmission_cd\", \"transmission_su\", \"transmission_cate\", \"transmission_ichi_cate\", \"register_ymd\",\n",
    "                       \"last_modified_ymd\"],\n",
    "                      [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\", \"equipment_cd\", \"equipment_nm\",\n",
    "                       \"equipment_jokyo_cd\", \"nm\", \"register_ymd\", \"last_modified_ymd\"]\n",
    "                      ]\n",
    "        # 読んだ後にとりあえず全部くっつけたい\n",
    "        co = 0\n",
    "        df_names = []\n",
    "        for ct in cat_list:\n",
    "            df_names.append(ct.rstrip(\".txt\")[4:])\n",
    "\n",
    "        start = time.time\n",
    "        dfs = []\n",
    "        cnt = 0\n",
    "        for cat, dn, dc in zip(cat_list, df_names, df_columns):\n",
    "            var = 'cat_' + str(dn)  # 変数名\n",
    "            tgt = '/home/tb1_taguchi_shoichi/py35_work/data/catalogue/' + str(cat)\n",
    "            # print(var)\n",
    "\n",
    "            if cat in [\"mc1_grade_sobi.txt\", \"mc1_grade_iro.txt\", \"mc1_brand_iro.txt\"]:\n",
    "                #要修正：ここだけ検証用に1万行縛りをなくす\n",
    "                #gl[var] = pd.read_csv(tgt, encoding=\"cp932\", delimiter=\"\\t\", names=dc, nrows=10000)\n",
    "                gl[var] = pd.read_csv(tgt, encoding=\"cp932\", delimiter=\"\\t\", low_memory=False, names=dc)\n",
    "            # 型指定がうまくいかなくてエラーになるので、すべてobjectで読み込む\n",
    "            else:\n",
    "                #gl[var] = dskdf.read_csv(tgt, encoding=\"cp932\", delimiter=\"\\t\", dtype=\"object\", names=dc)\n",
    "                gl[var] = dskdf.read_csv(tgt, encoding=\"cp932\", delimiter=\"\\t\", dtype=\"object\", names=dc)\n",
    "\n",
    "            cnt += 1\n",
    "            # 登録日、更新日は不要なので読み込み時点で落とす\n",
    "            # データ名もリストに格納しておく\n",
    "            dfs.append(var)\n",
    "\n",
    "        # 少しでも軽くするために、不要な変数を削除しておく\n",
    "        for dd in dfs:\n",
    "            gl[dd] = gl[dd].drop(\"register_ymd\", axis=1)\n",
    "            gl[dd] = gl[dd].drop(\"last_modified_ymd\", axis=1)\n",
    "            if dd == \"cat_grade\":\n",
    "                gl[dd] = gl[dd].drop(\"number_size\", axis=1)\n",
    "                gl[dd] = gl[dd].drop(\"catalogue_keisai_fuka_cate\", axis=1)\n",
    "            if dd == \"cat_mc_model\":\n",
    "                gl[dd] = gl[dd].drop(\"catalogue_keisai_fuka_cate\", axis=1)\n",
    "            if dd == \"cat_shashu\":\n",
    "                gl[dd] = gl[dd].drop(\"catalogue_keisai_fuka_cate\", axis=1)\n",
    "                gl[dd] = gl[dd].drop(\"search_key_1\", axis=1)\n",
    "                gl[dd] = gl[dd].drop(\"search_key_2\", axis=1)\n",
    "                gl[dd] = gl[dd].drop(\"search_key_3\", axis=1)\n",
    "                gl[dd] = gl[dd].drop(\"seo_folder_nm\", axis=1)\n",
    "            if dd == \"cat_sobi\":\n",
    "                gl[dd] = gl[dd].drop(\"equipment_hyoji_jun\", axis=1)\n",
    "\n",
    "        # 作成したデータは全て後ほど使うので、returnで返しておく\n",
    "    #     return dfs\n",
    "    # # カタログ全体のマージ準備\n",
    "    #\n",
    "    # def catalogue_merge_pre(self):\n",
    "        # 装備ごとに作られているテーブルが存在するので、装備をダミー変数化するためにpivotする必要がある\n",
    "        # ダミー化した変数の内容がわかるように、全てにsobi_の接頭辞をつけておく\n",
    "        cat_grade_sobi_pv = pd.pivot_table(cat_grade_sobi,\n",
    "                                                index=[\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\"],\n",
    "                                                columns=\"equipment_nm\", values=\"equipment_jokyo_cd\").reset_index()\n",
    "        col_equip = [\"sobi_\" + str(x) for x in cat_grade_sobi_pv.columns[5:, ]]\n",
    "        bs = ['brand_cd', 'shashu_cd', 'fmc_cd', 'mc_cd', 'grade_cd']\n",
    "        bs.extend(col_equip)\n",
    "        cat_grade_sobi_pv.columns = bs\n",
    "\n",
    "        # grade_iroは先にbrand_iroとマージして、基本色を獲得しておく\n",
    "        cat_grade_iro[\"count\"] = 1\n",
    "        cat_grade_brand_iro = pd.merge(cat_grade_iro, cat_brand_iro.ix[:, [\"brand_cd\", \"brand_color_cd\", \"base_color_cd\"]],\n",
    "                                       on=[\"brand_cd\", \"brand_color_cd\"])\n",
    "        sum_grade_brand_iro = pd.pivot_table(cat_grade_brand_iro,\n",
    "                                                  index=[\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\"],\n",
    "                                                  columns=\"base_color_cd\", values=\"count\").reset_index()\n",
    "\n",
    "        # ダミー化した変数の内容がわかるように、全てにbase_color_の接頭辞をつけておく\n",
    "        col_colors = [\"base_color_\" + str(x) for x in sum_grade_brand_iro.columns[5:, ]]\n",
    "        bs = ['brand_cd', 'shashu_cd', 'fmc_cd', 'mc_cd', 'grade_cd']\n",
    "        bs.extend(col_colors)\n",
    "        sum_grade_brand_iro.columns = bs\n",
    "\n",
    "    #     return [cat_grade_sobi_pv, sum_grade_brand_iro]\n",
    "    #\n",
    "    #\n",
    "    # # カタログ全体のマージ\n",
    "    # def catalogue_merge(self):\n",
    "        # 上記前処理が終わったら、ループで各種データをマージする\n",
    "        merge_key = [[\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\"], [\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\"],\n",
    "                     [\"brand_cd\", \"shashu_cd\", \"fmc_cd\"], [\"brand_cd\", \"shashu_cd\"], [\"brand_cd\", \"shashu_cd\"],\n",
    "                     [\"body_shape_cd\"], [\"transmission_cd\"], [\"brand_cd\"], [\"brand_cd\"], [\"brand_cd\"],\n",
    "                     [\"kanren_shashu_cate\"]\n",
    "                     ]\n",
    "\n",
    "        merge_data = [\"cat_grade\", \"cat_mc_model\", \"cat_fmc_model\", \"cat_shashu\", \"cat_kukuri_kanren_shashu\",\n",
    "                      \"cat_body_type\",\n",
    "                      \"cat_transmission_henkan\", \"cat_brand_country\", \"cat_brand_maker\", \"cat_brand\", \"cat_kanren_shashu\"\n",
    "                      ]\n",
    "\n",
    "\n",
    "        pandas_list = [\"mc1_body_type.txt\", \"mc1_kanren_shashu.txt\"]\n",
    "\n",
    "        # dask_ver\n",
    "        # a1 = cat_grade_sobi.merge(cat_grade_iro,on=[\"brand_cd\",\"shashu_cd\",\"fmc_cd\",\"mc_cd\",\"grade_cd\"])\n",
    "        # pandas_ver\n",
    "        moto = pd.merge(cat_grade_sobi_pv, sum_grade_brand_iro,\n",
    "                        on=[\"brand_cd\", \"shashu_cd\", \"fmc_cd\", \"mc_cd\", \"grade_cd\"])\n",
    "        # a1をdask化\n",
    "        a1 = dskdf.from_pandas(moto, npartitions=1)\n",
    "\n",
    "        # dask_ver\n",
    "        loops = np.array(list(range(11))) + 2\n",
    "        # print(loops)\n",
    "        #数字をキー、df名をバリューとする辞書を作成して対応\n",
    "        vals = [\"a\"+str(i) for i in loops]\n",
    "        df_dicts = {key: value for (key, value) in zip(loops, vals)}\n",
    "\n",
    "        for mk, md, lp in zip(merge_key, merge_data, loops):\n",
    "            if lp ==2:\n",
    "                gl[df_dicts[lp]] = a1.merge(gl[md], on=mk)\n",
    "            else:\n",
    "                gl[df_dicts[lp]] = gl[df_dicts[lp-1]].merge(gl[md], on=mk)\n",
    "\n",
    "        self.a12 = a12\n",
    "        # ファイルが死ぬほど大きいので退避させておく\n",
    "        self.a12.to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/catalogue/test/car_catalogue_ss-*.csv\")\n",
    "        return self.a12\n",
    "\n",
    "    def merge_shashu_to_base(self):\n",
    "        #車種データの作成\n",
    "        #要修正\n",
    "        self.body_summary, self.shashu_summary, self.shashu_fmc_summary, self.shashu_fmc_mc_summary, self.shashu_fmc_mc_grade_summary = self.toiawase_shukei_shashu()\n",
    "        self.seiyaku, self.bukken = self.make_seiyaku()\n",
    "        self.okini_shukei_zenkikan()\n",
    "        self.make_catalogue()\n",
    "        #マージ開始\n",
    "        #元データ\n",
    "\n",
    "        #デバッグのために、一旦２行くらいのデータを出力する\n",
    "        self.seiyaku.head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/seiyaku-*.csv\",columns=self.seiyaku.columns) #dask\n",
    "        self.body_summary.reset_index().head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/body_summary-*.csv\",columns=self.body_summary.reset_index().columns) #pandas\n",
    "        self.shashu_summary.reset_index().head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/shashu_summary-*.csv\",columns=self.shashu_summary.reset_index().columns) #pandas\n",
    "        self.shashu_fmc_summary.reset_index().head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/shashu_fmc_summary-*.csv\",columns=self.shashu_fmc_summary.reset_index().columns) #pandas\n",
    "        self.shashu_fmc_mc_summary.reset_index().head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/shashu_fmc_mc_summary-*.csv\",columns=self.shashu_fmc_mc_summary.reset_index().columns) #pandas\n",
    "        self.shashu_fmc_mc_grade_summary.reset_index().head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/shashu_fmc_mc_grade_summary-*.csv\",columns=self.shashu_fmc_mc_grade_summary.reset_index().columns) #pandas\n",
    "        self.okini_zen.head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/okini_zen-*.csv\",columns=self.okini_zen.columns) #daskをgroupby,reset_indexしたもの\n",
    "        self.a12.head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/a12-*.csv\",columns=self.a12.columns) #dask\n",
    "        self.koku_hoyu_area_cusdb.reset_index().head().to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/test/koku_hoyu_area_cusdb-*.csv\",columns=self.koku_hoyu_area_cusdb.reset_index().columns) #pandas\n",
    "\n",
    "        # +body_summary (key:body_type_cd)\n",
    "        self.body_summary = self.body_summary.reset_index()\n",
    "        self.body_summary = self.body_summary.rename(columns={\"a.body_type_name\":\"body_type_name\",\"count\":\"body_count\"})\n",
    "        # print(self.seiyaku[\"body_type_name\"])\n",
    "        a = self.seiyaku.merge(self.body_summary,on=\"body_type_name\")\n",
    "        taken_time(\"merge body summary\")\n",
    "        start = time.time()\n",
    "\n",
    "        # +shashu_summary (key:shashu_nm? shashu_brand_cd? shashu_ren_no?)\n",
    "        self.shashu_summary = self.shashu_summary.reset_index()\n",
    "        self.shashu_summary = self.shashu_summary.rename(columns={\"a.keisai_shashu_kj\":\"shashu\",\"count\":\"shashu_count\"})\n",
    "        b = a.merge(self.shashu_summary,on=\"shashu\")\n",
    "        taken_time(\"merge shashu summary\")\n",
    "        start = time.time()\n",
    "\n",
    "        # +shashu_fmc_summary (key:shashu_nm, fmc_cd)\n",
    "        self.shashu_fmc_summary = self.shashu_fmc_summary.reset_index()\n",
    "        self.shashu_fmc_summary = self.shashu_fmc_summary.rename(columns={\"a.keisai_shashu_kj\":\"shashu\",\"a.keisai_fmc_cd\":\"fmc_cd\",\"count\":\"shashu_fmc_count\"})\n",
    "        b = b.rename(columns={\"fmc\":\"fmc_cd\",\"count\":\"shashu_fmc_count\"})\n",
    "        c = b.merge(self.shashu_fmc_summary,on=[\"shashu\",\"fmc_cd\"])\n",
    "        taken_time(\"merge shashu_fmc summary\")\n",
    "        start = time.time()\n",
    "\n",
    "        #要修正_済み\n",
    "        # +shashu_fmc_mc_summary (key:shashu_nm, fmc_cd, mc_cd)\n",
    "        self.shashu_fmc_mc_summary = self.shashu_fmc_mc_summary.reset_index()\n",
    "        self.shashu_fmc_mc_summary = self.shashu_fmc_mc_summary.rename(columns={\"a.keisai_shashu_kj\":\"shashu\",\"a.keisai_fmc_cd\":\"fmc_cd\",\"a.keisai_mc_cd\":\"mc_cd\",\"count\":\"shashu_fmc_mc_count\"})\n",
    "        d = c.merge(self.shashu_fmc_mc_summary, on=[\"shashu\",\"fmc_cd\",\"mc_cd\"])\n",
    "        taken_time(\"merge shashu_fmc_mc summary\")\n",
    "        start = time.time()\n",
    "\n",
    "        # +shashu_fmc_mc_grade_summary (key:shashu_nm, fmc_cd, mc_cd, grade_cd)\n",
    "        self.shashu_fmc_mc_grade_summary = self.shashu_fmc_mc_grade_summary.reset_index()\n",
    "        self.shashu_fmc_mc_grade_summary = self.shashu_fmc_mc_grade_summary.rename(columns={\"a.keisai_shashu_kj\":\"shashu\",\"a.keisai_fmc_cd\":\"fmc_cd\",\"a.keisai_mc_cd\":\"mc_cd\",\"a.keisai_haikiryo_grade_cd\":\"grade_cd\",\"count\":\"shashu_fmc_mc_grade_count\"})\n",
    "        e = d.merge(self.shashu_fmc_mc_grade_summary, on=[\"shashu\",\"fmc_cd\",\"mc_cd\",\"grade_cd\"])\n",
    "        taken_time(\"merge shashu_fmc_mc_grade summary\")\n",
    "        start = time.time()\n",
    "\n",
    "        # +okini_zen (key:shashu, grade_cd)\n",
    "        # f = e.merge(self.okini_zen,on=[\"shashu\", \"grade_cd\"])\n",
    "        f = e.merge(self.okini_zen,on=[\"shashu\", \"grade_cd\"])\n",
    "        taken_time(\"merge okini_zen\")\n",
    "        start = time.time()\n",
    "\n",
    "        # +a12(カタログ) (key:shashu, fmc_cd,mc_cd, grade_cd)\n",
    "        #一旦コメントアウトして実行\n",
    "        self.a12.rename(columns={\"shashu_nm\":\"shashu\"})\n",
    "        g = f.merge(self.a12,on=[\"shashu\",\"fmc_cd\",\"mc_cd\",\"grade_cd\"])\n",
    "        taken_time(\"merge catalogue\")\n",
    "        start = time.time()\n",
    "\n",
    "        #期間別はデータ量が膨れ上がるので今回は諦め\n",
    "        # +okini_kikanbetsu (key:shashu_nm, grade_cd)\n",
    "\n",
    "        #chihoでマージするとデータ粒度が落ちない？\n",
    "        #そもそも成約データに市区町村が存在している？\n",
    "\n",
    "        self.car_raw_data = g.merge(self.koku_hoyu_area_cusdb,on=\"chiho\")\n",
    "        taken_time(\"merge koku_hoyu_area_cusdb\")\n",
    "        start = time.time()\n",
    "\n",
    "        return self.car_raw_data\n",
    "\n",
    "def main():\n",
    "    #地域\n",
    "    #全体：国勢調査、保有台数、問い合わせ、カスタマDB\n",
    "    fnc = rawdata()\n",
    "    fnc.make_kokusei()\n",
    "    taken_time(\"kokusei\")\n",
    "    start = time.time()\n",
    "    fnc.make_hoyu()\n",
    "    taken_time(\"make hoyu\")\n",
    "    start = time.time()\n",
    "    fnc.make_toiawase()\n",
    "    taken_time(\"make toiawase\")\n",
    "    start = time.time()\n",
    "    fnc.toiawase_shukei_area()\n",
    "    taken_time(\"make shukei area\")\n",
    "    start = time.time()\n",
    "    fnc.summarize_cusdb()\n",
    "    taken_time(\"summarize cusdb\")\n",
    "    start = time.time()\n",
    "    #上記の実施結果としてkoku_hoyu_area_cusdbが出力される\n",
    "\n",
    "    #車種\n",
    "    #全体：問い合わせ、お気に入り、カタログ\n",
    "    #車種・グレード別集計（問い合わせ）\n",
    "    fnc.merge_shashu_to_base()\n",
    "    \n",
    "    gc.enable()\n",
    "    del self.seiyaku.merge, self.body_summary, self.shashu_summary, self.shashu_fmc_summary, self.shashu_fmc_mc_summary,\n",
    "    self.shashu_fmc_mc_grade_summary, self.okini_zen, self.a12, self.koku_hoyu_area_cusdb\n",
    "    gc.collect()\n",
    "    \n",
    "    return fnc.car_raw_data\n",
    "\n",
    "res = main()\n",
    "\n",
    "res.to_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/car_raw_data-*.csv\")\n",
    "taken_time(\"make raw csv\")\n",
    "start = time.time()\n",
    "\n",
    "#pickle fileの作成\n",
    "pic = pd.read_csv(\"/home/tb1_taguchi_shoichi/py35_work/data/output/car_raw_data-*.csv\")\n",
    "with open('/home/tb1_taguchi_shoichi/py35_work/data/output/car_raw_d.pkl','wb') as f1:\n",
    "    pickle.dump(pic,f1)\n",
    "taken_time(\"make pickle\")\n",
    "start = time.time()\n",
    "del pic\n",
    "gc.collect()\n",
    "\n",
    "#試しに読み込み時間も測定\n",
    "with open('/home/tb1_taguchi_shoichi/py35_work/data/output/car_raw_d.pkl','rb') as f2:\n",
    "    data=pickle.load(f2)\n",
    "taken_time(\"read pickle\")\n",
    "\n",
    "print(res.head(2))\n",
    "print(\"finished!\")\n",
    "\n",
    "#デバッグに時間掛かる\n",
    "# 　→部分的に実行することはできない？\n",
    "\n",
    "# バージョン管理をしたい\n",
    "# 　→gitとの連携？\n",
    "\n",
    "#変数加工方針?\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
